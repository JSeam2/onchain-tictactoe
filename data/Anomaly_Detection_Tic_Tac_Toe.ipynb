{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKP_h_Q61LZZ"
   },
   "source": [
    "# Tic Tac Toe Verifier\n",
    "\n",
    "We create an ML model that verifies TicTacToe Games\n",
    "\n",
    "Make sure to use a GPU otherwise you'll take a super long time to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kbmq0MNjU3-g",
    "outputId": "7d12599f-530d-4971-a625-02fe42a6f11e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in /Users/jseam/Github/jseam2ezkl/.env/lib/python3.11/site-packages (1.14.1)\n",
      "Requirement already satisfied: numpy in /Users/jseam/Github/jseam2ezkl/.env/lib/python3.11/site-packages (from onnx) (1.24.3)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /Users/jseam/Github/jseam2ezkl/.env/lib/python3.11/site-packages (from onnx) (4.24.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /Users/jseam/Github/jseam2ezkl/.env/lib/python3.11/site-packages (from onnx) (4.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZawMT3tcHvC"
   },
   "source": [
    "# Generate legal game states and illegal game states\n",
    "\n",
    "1. Recursively lookup all the possible tictactoe game states and create a legal dataset\n",
    "2. Use the legal dataset and permute illegal game states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vd-XNueUcG-r"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def check_winner(board):\n",
    "    winning_combinations = [\n",
    "        [0, 1, 2], [3, 4, 5], [6, 7, 8],  # rows\n",
    "        [0, 3, 6], [1, 4, 7], [2, 5, 8],  # columns\n",
    "        [0, 4, 8], [2, 4, 6]              # diagonals\n",
    "    ]\n",
    "    for combo in winning_combinations:\n",
    "        if board[combo[0]] == board[combo[1]] == board[combo[2]] and board[combo[0]] is not None:\n",
    "            return board[combo[0]]\n",
    "    return None\n",
    "\n",
    "def generate_games(board, player):\n",
    "    winner = check_winner(board)\n",
    "    if winner or None not in board:\n",
    "        # Game is over, save the outcome and board state\n",
    "        return [{\n",
    "            \"history\": [list(board)],\n",
    "            \"outcome\": winner if winner else \"Draw\"\n",
    "        }]\n",
    "\n",
    "    games = []\n",
    "    for i in range(9):\n",
    "        if board[i] is None:\n",
    "            new_board = board.copy()\n",
    "            new_board[i] = player\n",
    "            next_player = 'O' if player == 'X' else 'X'\n",
    "            next_games = generate_games(new_board, next_player)\n",
    "            for game in next_games:\n",
    "                game[\"history\"].insert(0, list(board))\n",
    "            games.extend(next_games)\n",
    "\n",
    "    return games\n",
    "\n",
    "def generate_illegal_games(games):\n",
    "    illegal_games = []\n",
    "    for game in games:\n",
    "        history = game['history']\n",
    "        illegal_history = []\n",
    "        for round in history:\n",
    "            round_list = []\n",
    "            for item in round:\n",
    "                # cycle the permutations\n",
    "                if item is None:\n",
    "                    round_list.append(\"X\")\n",
    "                if item == \"X\":\n",
    "                    round_list.append(\"O\")\n",
    "                if item == \"O\":\n",
    "                    round_list.append(None)\n",
    "            illegal_history.append(round_list)\n",
    "        illegal_games.append({\n",
    "            \"history\": illegal_history,\n",
    "            \"outcome\": game[\"outcome\"]\n",
    "        })\n",
    "    return illegal_games\n",
    "\n",
    "\n",
    "\n",
    "initial_board = [None for _ in range(9)]\n",
    "games = generate_games(initial_board, 'X')\n",
    "illegal_games = generate_illegal_games(games)\n",
    "\n",
    "with open(\"tic_tac_toe_games_good.json\", \"w\") as file:\n",
    "    file.write(\"[\\n\")  # Start of the list\n",
    "    for i, game in enumerate(games):\n",
    "        json.dump(game, file, separators=(',', ': '))\n",
    "        if i != len(games) - 1:  # If it's not the last game, add a comma\n",
    "            file.write(\",\\n\")\n",
    "        else:\n",
    "            file.write(\"\\n\")\n",
    "    file.write(\"]\\n\")\n",
    "\n",
    "with open(\"tic_tac_toe_games_bad.json\", \"w\") as file:\n",
    "    file.write(\"[\\n\")  # Start of the list\n",
    "    for i, game in enumerate(illegal_games):\n",
    "        json.dump(game, file, separators=(',', ': '))\n",
    "        if i != len(illegal_games) - 1:  # If it's not the last game, add a comma\n",
    "            file.write(\",\\n\")\n",
    "        else:\n",
    "            file.write(\"\\n\")\n",
    "    file.write(\"]\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FTtqoHs13Y_"
   },
   "source": [
    "# Generate the Anomaly Detection Model\n",
    "1. Create a variational autoencoder, as the state space is small enough put the game history into an input matrix\n",
    "\n",
    "2. Fully connect the final outputs and use two outputs to represent true or false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_yV7ou8B1LDG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, IterableDataset, random_split\n",
    "import torch.optim as optim\n",
    "import json\n",
    "\n",
    "\n",
    "class TicTacToeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TicTacToeNet, self).__init__()\n",
    "\n",
    "        # Fully connected layers\n",
    "        # 9 possible inputs x 10 rounds + 1 final output value\n",
    "        self.fc0 = nn.Linear(91, 16)\n",
    "        self.fc1 = nn.Linear(16, 3)\n",
    "        self.fc2 = nn.Linear(3, 16)\n",
    "        self.fc3 = nn.Linear(16, 91)\n",
    "        self.fc4 = nn.Linear(91, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through fully connected layers with ReLU activation\n",
    "        x = torch.relu(self.fc0(x))\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qnf8wZk3nnou"
   },
   "source": [
    "## Load all possible tictactoe games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "Sw19C2VxZpS9",
    "outputId": "d4caf07a-c297-4296-9350-507cafca2e38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good ([2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 0, 1, 0, 2, 2, 2, 2, 2, 2, 0, 1, 0, 1, 2, 2, 2, 2, 2, 0, 1, 0, 1, 0, 2, 2, 2, 2, 0, 1, 0, 1, 0, 1, 2, 2, 2, 0, 1, 0, 1, 0, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0], [1, 0])\n",
      "bad ([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 2, 0, 0, 2, 1, 1, 0, 1, 0, 2, 0, 2, 2, 1, 1, 0, 1, 0, 2, 0, 2, 2, 1, 1, 0, 1, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0], [0, 1])\n",
      "510336\n"
     ]
    }
   ],
   "source": [
    "class JsonDataset(IterableDataset):\n",
    "    def __init__(self, file_good, file_bad):\n",
    "        self.file_good = file_good\n",
    "        self.file_bad = file_bad\n",
    "        self.data = self.load_data()\n",
    "\n",
    "\n",
    "\n",
    "    def parse_json_object(self, line):\n",
    "        try:\n",
    "            return json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "\n",
    "    def encode_board(self, board):\n",
    "        encoding = []\n",
    "        for cell in board:\n",
    "            if cell == 'X':\n",
    "                encoding.extend([0])\n",
    "            elif cell == 'O':\n",
    "                encoding.extend([1])\n",
    "            else:\n",
    "                encoding.extend([2])\n",
    "\n",
    "        return encoding\n",
    "\n",
    "    def flatten_list(self, matrix):\n",
    "        flat_list = []\n",
    "        for row in matrix:\n",
    "            flat_list.extend(row)\n",
    "        return flat_list\n",
    "\n",
    "\n",
    "\n",
    "    def encode_outcome(self, outcome):\n",
    "        if outcome == 'X':\n",
    "            return 0\n",
    "        elif outcome == 'O':\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        padded_history, sample_outcome = self.data[idx]\n",
    "        return torch.tensor(padded_history, dtype=torch.float), torch.tensor(sample_outcome, dtype=torch.float)\n",
    "\n",
    "    def load_data_type(self, file, valid):\n",
    "        data = []\n",
    "        with open(file, 'r') as f:\n",
    "            # Skip the first line (which is \"[\")\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                # Remove the trailing comma for all lines except the last one (which is \"]\")\n",
    "                if line.endswith(\",\\n\"):\n",
    "                    line = line[:-2]\n",
    "                sample = self.parse_json_object(line)\n",
    "                if sample is not None:\n",
    "                    max_length = 10  # Maximum length of a Tic Tac Toe game\n",
    "                    history = sample['history']\n",
    "\n",
    "                    if len(history) == max_length:\n",
    "                        padded_history = history\n",
    "                    else:\n",
    "                        padded_history = history + [[None] * 9 for _ in range(max_length - len(history))]\n",
    "\n",
    "                    padded_history = [self.encode_board(x) for x in padded_history]\n",
    "                    padded_history = self.flatten_list(padded_history)\n",
    "                    sample_outcome = self.encode_outcome(sample['outcome'])\n",
    "\n",
    "                    padded_history.extend([sample_outcome])\n",
    "\n",
    "                    data.append((padded_history, valid))\n",
    "        return data\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        data = []\n",
    "        data.extend(self.load_data_type(self.file_good, [1, 0]))\n",
    "        data.extend(self.load_data_type(self.file_bad, [0, 1]))\n",
    "        print(\"good\", data[0])\n",
    "        print(\"bad\", data[400000])\n",
    "        return data\n",
    "\n",
    "dataset = JsonDataset('tic_tac_toe_games_good.json', 'tic_tac_toe_games_bad.json')  # Add other files as necessary\n",
    "\n",
    "total_size = len(dataset)\n",
    "print(total_size)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6VBZchsnx2U"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "NE1QPy_91z_C",
    "outputId": "df2da524-eeb5-45d9-c3cc-b6e83450c715"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m valid \u001b[38;5;241m=\u001b[39m valid\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Move outcome to CUDA\u001b[39;00m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, valid)\n\u001b[1;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Github/jseam2ezkl/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m, in \u001b[0;36mTicTacToeNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Pass through fully connected layers with ReLU activation\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[1;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n",
      "File \u001b[0;32m~/Github/jseam2ezkl/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Github/jseam2ezkl/.env/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = TicTacToeNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "MAX_EPOCH = 5\n",
    "\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    model.train()\n",
    "    for data, valid in train_loader:\n",
    "        data = data.to(device)  # Move history to CUDA\n",
    "        valid = valid.to(device)  # Move outcome to CUDA\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "\n",
    "        loss = criterion(outputs, valid)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for data, valid in test_loader:\n",
    "\n",
    "            data = data.to(device)  # Move history to CUDA\n",
    "            valid = valid.to(device)  # Move outcome to CUDA\n",
    "            outputs = model(data)\n",
    "\n",
    "            # Get the predicted class (the index of the maximum value in the output tensor)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, valid = torch.max(valid, 1)\n",
    "\n",
    "\n",
    "            total_predictions += valid.size(0)\n",
    "            correct_predictions += (predicted == valid).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct_predictions / total_predictions\n",
    "    print(f\"Epoch {epoch + 1}/{MAX_EPOCH} - Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLEdsCI2od7x"
   },
   "source": [
    "## Export Onnx and Data out for the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jb4hgLGanF9J",
    "outputId": "d533aeac-4079-4bfd-cc05-4a2b1678b547"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.,  ..., 0., 1., 0.],\n",
      "        [2., 2., 2.,  ..., 2., 2., 1.],\n",
      "        [2., 2., 2.,  ..., 1., 1., 2.],\n",
      "        ...,\n",
      "        [2., 2., 2.,  ..., 2., 2., 0.],\n",
      "        [0., 0., 0.,  ..., 2., 2., 0.],\n",
      "        [0., 0., 0.,  ..., 2., 2., 1.]])\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "{'input_data': [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 1.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 2.0, 1.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0]]}\n"
     ]
    }
   ],
   "source": [
    "# Move the model to the CPU\n",
    "model = model.cpu()\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Obtain a sample datapoint from the DataLoader\n",
    "sample_data_iter = iter(train_loader)\n",
    "sample_history, _ = next(sample_data_iter)\n",
    "\n",
    "# No need to move the sample to the device since everything is on the CPU now\n",
    "x = sample_history\n",
    "print(x)\n",
    "\n",
    "# Export the model using ONNX\n",
    "torch.onnx.export(\n",
    "    model,                        # model being run\n",
    "    x,                            # model input (or a tuple for multiple inputs)\n",
    "    \"tictactoe_network.onnx\",     # where to save the model (can be a file or file-like object)\n",
    "    export_params=True,           # store the trained parameter weights inside the model file\n",
    "    opset_version=10,             # the ONNX version to export the model to\n",
    "    do_constant_folding=True,     # whether to execute constant folding for optimization\n",
    "    input_names=['input'],        # the model's input names\n",
    "    output_names=['output'],      # the model's output names\n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size'},     # variable length axes\n",
    "        'output': {0: 'batch_size'}\n",
    "    }\n",
    ")\n",
    "\n",
    "data_array = ((x[-1]).detach().numpy()).reshape([-1]).tolist()\n",
    "\n",
    "data = dict(input_data = [data_array])\n",
    "\n",
    "print(data)\n",
    "\n",
    "    # Serialize data into file:\n",
    "json.dump(data, open(\"data.json\", 'w'))\n",
    "\n",
    "\n",
    "# use the test set to calibrate the circuit\n",
    "cal_data = dict(input_data = x.flatten().tolist())\n",
    "\n",
    "# Serialize calibration data into file:\n",
    "json.dump(data, open(\"cal_data.json\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cVKOwNkeVS_L",
    "outputId": "286d0e54-d43e-4c7b-b625-b7b0ff3b3b8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ezkl==2.5.0 in /Users/jseam/Github/jseam2ezkl/.env/lib/python3.11/site-packages (2.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ezkl==2.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "XciCNe7FuJrP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ezkl\n",
    "\n",
    "data_path = os.path.join(\"data.json\")\n",
    "calibration_path = os.path.join(\"cal_data.json\")\n",
    "model_path = os.path.join('tictactoe_network.onnx')\n",
    "compiled_model_path = os.path.join('network.compiled')\n",
    "pk_path = os.path.join('test.pk')\n",
    "vk_path = os.path.join('test.vk')\n",
    "settings_path = os.path.join('settings.json')\n",
    "srs_path = os.path.join('kzg.srs')\n",
    "witness_path = os.path.join('witness.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "vhuNewTIt_Si"
   },
   "outputs": [],
   "source": [
    "res = ezkl.gen_settings(model_path, settings_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "IJdL0ywquSa5"
   },
   "outputs": [],
   "source": [
    "res = await ezkl.calibrate_settings(data_path, model_path, settings_path, \"resources\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Pxr8u1NrxJ6M"
   },
   "outputs": [],
   "source": [
    "res = ezkl.compile_circuit(model_path, compiled_model_path, settings_path)\n",
    "assert res == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "jCnJ-SV0xVrx"
   },
   "outputs": [],
   "source": [
    "res = ezkl.get_srs(srs_path, settings_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "e5Wem6vsxYqm"
   },
   "outputs": [],
   "source": [
    "res = ezkl.gen_witness(data_path, compiled_model_path, witness_path)\n",
    "assert os.path.isfile(witness_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "88bQephvxcM4"
   },
   "outputs": [],
   "source": [
    "res = ezkl.mock(witness_path, compiled_model_path)\n",
    "assert res == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "VSo9lFeDPRRM"
   },
   "outputs": [],
   "source": [
    "res = ezkl.setup(\n",
    "        compiled_model_path,\n",
    "        vk_path,\n",
    "        pk_path,\n",
    "        srs_path,\n",
    "    )\n",
    "\n",
    "assert res == True\n",
    "assert os.path.isfile(vk_path)\n",
    "assert os.path.isfile(pk_path)\n",
    "assert os.path.isfile(settings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CScshU1sPTj1",
    "outputId": "95ad7601-0349-4663-b22b-2e23856dab34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instances': [[[5432626032756654017, 1961834588764195904, 11835134460333605139, 1680038075927467451], [17322468249157237758, 16138382668123003341, 5640928463521831588, 1008128559805110475]]], 'proof': '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', 'transcript_type': 'EVM'}\n"
     ]
    }
   ],
   "source": [
    "proof_path = os.path.join('test.pf')\n",
    "\n",
    "res = ezkl.prove(\n",
    "        witness_path,\n",
    "        compiled_model_path,\n",
    "        pk_path,\n",
    "        proof_path,\n",
    "        srs_path,\n",
    "        \"single\",\n",
    "    )\n",
    "\n",
    "print(res)\n",
    "assert os.path.isfile(proof_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ITuSThGPPVKd",
    "outputId": "d1d9e903-ef00-49bf-e4e4-9217cbf6b144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified\n"
     ]
    }
   ],
   "source": [
    "# VERIFY IT\n",
    "res = ezkl.verify(\n",
    "        proof_path,\n",
    "        settings_path,\n",
    "        vk_path,\n",
    "        srs_path,\n",
    "    )\n",
    "\n",
    "assert res == True\n",
    "print(\"verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PA-Pn-k7PXDV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
